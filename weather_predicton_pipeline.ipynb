{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: WorldWeatherPy in /home/jovyan/.local/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.0.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (0.23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from WorldWeatherPy->-r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 4)) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy->-r requirements.txt (line 1)) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy->-r requirements.txt (line 1)) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy->-r requirements.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy->-r requirements.txt (line 1)) (2.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: WorldWeatherPy in /home/jovyan/.local/lib/python3.8/site-packages (0.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from WorldWeatherPy) (2.28.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from WorldWeatherPy) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->WorldWeatherPy) (2022.5)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from pandas->WorldWeatherPy) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->WorldWeatherPy) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy) (2021.5.30)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->WorldWeatherPy) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --user -r requirements.txt\n",
    "!pip install --user WorldWeatherPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "block:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parametres(out_file):\n",
    "    \n",
    "    api_key = 'bd35020cdd3643f4b69142436222912'\n",
    "    attributes = ['date','time', 'moon_illumination', \n",
    "              'tempC', 'tempF', 'windspeedMiles', 'windspeedKmph', 'winddirDegree', 'weatherCode',\n",
    "              'weatherDesc', 'precipMM', 'precipInches', 'humidity', 'visibility', 'visibilityMiles', \n",
    "              'pressure', 'pressureInches', 'cloudcover', 'HeatIndexC', 'HeatIndexF', 'DewPointC', 'DewPointF', 'WindChillC', \n",
    "              'WindChillF', 'WindGustMiles', 'WindGustKmph', 'FeelsLikeC', 'FeelsLikeF', 'uvIndex']\n",
    "    \n",
    "    conditions = ['Sunny','Clear','Cloudy','Rain','Snow']\n",
    "    location_list = ['milan','turin','florence','bologna','rome','naples','palermo']\n",
    "\n",
    "    frequency = 6\n",
    "    start_date = '2018-1-1' \n",
    "    end_date = '2023-1-1'\n",
    "    \n",
    "    parametres = {'api_key' : api_key,\n",
    "            'attributes' : attributes,\n",
    "            'conditions' : conditions,\n",
    "            'location_list' : location_list,\n",
    "            'frequency' : frequency,\n",
    "            'start_date' : start_date,\n",
    "            'end_date' : end_date}\n",
    "\n",
    "    # Creates a json object based on `parametres`\n",
    "    parametres_json = json.dumps(parametres)\n",
    "\n",
    "\n",
    "    # Saves the json object into a file\n",
    "    with open(out_file, 'w') as f:\n",
    "        json.dump(parametres_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "block:milan_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#from wwo_hist import retrieve_hist_data\n",
    "from WorldWeatherPy import DetermineListOfAttributes\n",
    "from WorldWeatherPy import HistoricalLocationWeather\n",
    "from WorldWeatherPy import RetrieveByAttribute\n",
    "import pandas as pd \n",
    "\n",
    "def download_milan_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][0] #milan\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    \n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "\n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "block:turin_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from WorldWeatherPy import DetermineListOfAttributes\n",
    "from WorldWeatherPy import HistoricalLocationWeather\n",
    "from WorldWeatherPy import RetrieveByAttribute\n",
    "import pandas as pd \n",
    "\n",
    "def download_turin_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][1] # turin\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "            \n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "block:florence_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from WorldWeatherPy import DetermineListOfAttributes\n",
    "from WorldWeatherPy import HistoricalLocationWeather\n",
    "from WorldWeatherPy import RetrieveByAttribute\n",
    "import pandas as pd \n",
    "\n",
    "def download_florence_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][2] # florence\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "            \n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "block:bologna_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from WorldWeatherPy import DetermineListOfAttributes\n",
    "from WorldWeatherPy import HistoricalLocationWeather\n",
    "from WorldWeatherPy import RetrieveByAttribute\n",
    "import pandas as pd \n",
    "\n",
    "def download_bologna_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][3] # bologna\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "\n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "block:rome_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from WorldWeatherPy import DetermineListOfAttributes\n",
    "from WorldWeatherPy import HistoricalLocationWeather\n",
    "from WorldWeatherPy import RetrieveByAttribute\n",
    "import pandas as pd \n",
    "\n",
    "def download_rome_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][4] # rome\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "            \n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "block:naples_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from WorldWeatherPy import DetermineListOfAttributes\n",
    "from WorldWeatherPy import HistoricalLocationWeather\n",
    "from WorldWeatherPy import RetrieveByAttribute\n",
    "import pandas as pd \n",
    "\n",
    "def download_naples_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][5] # naples\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "\n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "block:palermo_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from WorldWeatherPy import DetermineListOfAttributes\n",
    "from WorldWeatherPy import HistoricalLocationWeather\n",
    "from WorldWeatherPy import RetrieveByAttribute\n",
    "import pandas as pd \n",
    "\n",
    "def download_palermo_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][6] # palermo\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "                \n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "block:merge_data",
     "prev:milan_data",
     "prev:turin_data",
     "prev:florence_data",
     "prev:bologna_data",
     "prev:rome_data",
     "prev:naples_data",
     "prev:palermo_data"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_data(parametres_file):\n",
    "    \n",
    "    download_milan_data(parametres_file)\n",
    "    download_turin_data(parametres_file)\n",
    "    download_florence_data(parametres_file)\n",
    "    download_bologna_data(parametres_file)\n",
    "    download_rome_data(parametres_file)\n",
    "    download_naples_data(parametres_file)\n",
    "    download_palermo_data(parametres_file)\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "        \n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location_list = parametres['location_list']\n",
    "    #data = [None] * len(location_list)\n",
    "\n",
    "    #for i, location in enumerate(location_list):\n",
    "        #data[i] = pd.read_csv(f'datavol-1/data/{location}.csv')\n",
    "        \n",
    "    data = [pd.read_csv(f'data/{location}.csv') for location in location_list]    \n",
    "    \n",
    "    \n",
    "    # creazione della tabella finale utilizzando il metodo concat()\n",
    "    merged_table = pd.concat([data[0], data[1], data[2], data[3], data[4], data[5], data[6]])\n",
    "    merged_table[ ['date','time', 'moon_illumination', \n",
    "              'tempC', 'tempF', 'windspeedMiles', 'windspeedKmph', 'winddirDegree', 'weatherCode', \n",
    "              'weatherDesc', 'precipMM', 'precipInches', 'humidity', 'visibility', 'visibilityMiles', \n",
    "              'pressure', 'pressureInches', 'cloudcover', 'HeatIndexC', 'HeatIndexF', 'DewPointC', 'DewPointF', 'WindChillC', \n",
    "              'WindChillF', 'WindGustMiles', 'WindGustKmph', 'FeelsLikeC', 'FeelsLikeF', 'uvIndex']] = merged_table[ ['date','time','moon_illumination', \n",
    "                                                                                                                      'tempC', 'tempF', 'windspeedMiles', 'windspeedKmph', 'winddirDegree', 'weatherCode', \n",
    "                                                                                                                      'weatherDesc', 'precipMM', 'precipInches', 'humidity', 'visibility', 'visibilityMiles', \n",
    "                                                                                                                      'pressure', 'pressureInches', 'cloudcover', 'HeatIndexC', 'HeatIndexF', 'DewPointC', 'DewPointF', 'WindChillC', \n",
    "                                                                                                                      'WindChillF', 'WindGustMiles', 'WindGustKmph', 'FeelsLikeC', 'FeelsLikeF', 'uvIndex']].replace(-0,0)\n",
    "    merged_table = merged_table.reset_index(drop=True)\n",
    "    merged_table.to_csv('data/merged_table.csv', index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PB2oCORZO_7O",
    "tags": [
     "block:data_preprocessing",
     "prev:merge_data"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump\n",
    "\n",
    "def data_preprocessing():\n",
    "    # lettura di un file CSV in un DataFrame\n",
    "    df = pd.read_csv('data/merged_table.csv')\n",
    "\n",
    "    #df = df.drop(['date','time'], axis=1) #La città potrebbe essere una feature più importante, poiché il meteo può variare significativamente da una città all'altra\n",
    "    df = df.drop(['date','time','city'], axis=1) # Tuttavia, se si utilizza un modello di Random Forest su un dataset di diverse città, il modello potrebbe essere in grado di apprendere autonomamente queste differenze e quindi la colonna 'city' potrebbe non essere necessaria.\n",
    "\n",
    "    '''\n",
    "    factor = pd.factorize(df['city'])\n",
    "    city_values = factor[0]\n",
    "    city_definitions = factor[1]\n",
    "\n",
    "    df['city'] = city_values\n",
    "    '''\n",
    "\n",
    "    y = df[['weatherDesc']]\n",
    "    x = df.drop(['weatherDesc'], axis=1)\n",
    "    columns = x.columns\n",
    "\n",
    "    # Standardize the data in X using a scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x)\n",
    "    X = scaler.transform(x)\n",
    "    features = pd.DataFrame(X, columns = columns)\n",
    "    \n",
    "    dump(scaler, 'models/scaler.joblib')\n",
    "    \n",
    "    return features, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "block:hyperparametres"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def hyperparametres(out_file):\n",
    "    \n",
    "    n_estimators = 400\n",
    "    bootstrap = True\n",
    "    max_features = 'sqrt'\n",
    "    criterion = ['gini','entropy']\n",
    "    min_samples_split = 2 \n",
    "    min_samples_leaf = 1\n",
    "    max_depth = 3\n",
    "\n",
    "    penalty = ['l1','l2']\n",
    "    C = 2.0\n",
    "    solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag']\n",
    "    max_iter = 2000\n",
    "\n",
    "    hyperparametres = {'n_estimators' : n_estimators,\n",
    "            'bootstrap' : bootstrap,\n",
    "            'max_features' : max_features,\n",
    "            'criterion' : criterion,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'penalty' : penalty,\n",
    "            'C' : C,\n",
    "            'solver' : solver,\n",
    "            'max_iter' : max_iter\n",
    "             }\n",
    "\n",
    "    # Creates a json object based on `parametres`\n",
    "    hyperparametres_json = json.dumps(hyperparametres)\n",
    "\n",
    "\n",
    "    # Saves the json object into a file\n",
    "    with open(out_file, 'w') as f:\n",
    "        json.dump(hyperparametres_json, f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "block:decision_tree",
     "prev:data_preprocessing"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump\n",
    "\n",
    "def decision_tree(hyperparametres_file):\n",
    "\n",
    "    features, y = data_preprocessing()\n",
    "\n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(hyperparametres_file) as f:\n",
    "        hyperparametres = json.load(f)\n",
    "        \n",
    "    hyperparametres = json.loads(hyperparametres)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, y, test_size = 0.3)\n",
    "\n",
    "    criterion = hyperparametres['criterion'][1] #entropy\n",
    "\n",
    "    model = DecisionTreeClassifier(max_depth = hyperparametres['max_depth'], criterion = criterion)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the model\n",
    "    dump(model, 'models/decision_tree.joblib')\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "block:random_forest",
     "prev:data_preprocessing"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import dump\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def random_forest(hyperparametres_file):\n",
    "    \n",
    "    features, y = data_preprocessing()\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(hyperparametres_file) as f:\n",
    "        hyperparametres = json.load(f)\n",
    "        \n",
    "    hyperparametres = json.loads(hyperparametres)\n",
    "    \n",
    "    criterion = hyperparametres['criterion'][1] #entropy\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, y, test_size = 0.3)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    model = RandomForestClassifier(n_estimators = hyperparametres['n_estimators'], max_features = hyperparametres['max_features'], criterion = criterion, max_depth = hyperparametres['max_depth'] )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the model\n",
    "    dump(model, 'models/random_forest_model_entropy.joblib')\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "block:logistic_regression",
     "prev:data_preprocessing"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump\n",
    "\n",
    "def logistic_regression(hyperparametres_file):\n",
    "\n",
    "    features, y = data_preprocessing()\n",
    "\n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(hyperparametres_file) as f:\n",
    "        hyperparametres = json.load(f)\n",
    "        \n",
    "    hyperparametres = json.loads(hyperparametres)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, y, test_size = 0.3)\n",
    "\n",
    "    penalty = hyperparametres['penalty'][1] #l2\n",
    "    solver = hyperparametres['solver'][0] #newton-cg\n",
    "\n",
    "    model = LogisticRegression(penalty = penalty , C = hyperparametres['C'], solver = solver , max_iter = hyperparametres['max_iter'])\n",
    "    #model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the model\n",
    "    dump(model, 'models/logistic_regression.joblib')\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "block:show_results",
     "prev:random_forest",
     "prev:hyperparametres",
     "prev:decision_tree",
     "prev:logistic_regression"
    ]
   },
   "outputs": [],
   "source": [
    "def show_results(accuracy_file, hyperparametres_file, accuracy_dc, accuracy_rf, accuracy_lr):\n",
    "    # Given the outputs from decision_tree and logistic regression components\n",
    "    # the results are shown.\n",
    "    # Save output into file\n",
    "\n",
    "\n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(hyperparametres_file) as f:\n",
    "        hyperparametres = json.load(f)\n",
    "        \n",
    "    hyperparametres = json.loads(hyperparametres)\n",
    "\n",
    "\n",
    "    with open(accuracy_file, 'w') as f:\n",
    "        f.write('Decision treee(accuracy): ' + str(accuracy_dc) + '  max_depth: ' +  str(hyperparametres['max_depth']) +  ' criterion: ' + hyperparametres['criterion'][1] + '\\n')\n",
    "        f.write('Random forest(accuracy): ' + str(accuracy_rf) + '  n_estimators: ' + str(hyperparametres['n_estimators']) + ' max_features: ' + str(hyperparametres['max_features']) + ' criterion: ' + hyperparametres['criterion'][1] + '\\n')\n",
    "        f.write('Logistic_regression(accuracy): ' + str(accuracy_lr) + '  penalty: '  + hyperparametres['penalty'][1] + ' C: ' + str(hyperparametres['C']) + ' solver: '  + hyperparametres['solver'][2] + ' max_iter: '+ str(hyperparametres['max_iter']) + '\\n')\n",
    "\n",
    "    print(f\"'Decision treee(accuracy): {accuracy_dc} \")\n",
    "    print(f\"Random forest(accuracy): {accuracy_rf} \")\n",
    "    print(f\"Logistic_regression(accuracy): {accuracy_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "block:classify",
     "prev:parametres",
     "prev:merge_data",
     "prev:random_forest",
     "prev:show_results",
     "prev:hyperparametres",
     "prev:decision_tree",
     "prev:logistic_regression"
    ]
   },
   "outputs": [],
   "source": [
    "def classify():\n",
    "    parametres_file = 'parametres.txt'\n",
    "    hyperparametres_file = 'hyperparametres.txt'\n",
    "    accuracy_file = 'results/accuracy.txt'\n",
    "    \n",
    "    parametres(parametres_file)\n",
    "    merge_data(parametres_file)\n",
    "    hyperparametres(hyperparametres_file)\n",
    "    \n",
    "    accuracy_dc = decision_tree(hyperparametres_file)\n",
    "    accuracy_rf = random_forest(hyperparametres_file)\n",
    "    accuracy_lr = logistic_regression(hyperparametres_file)\n",
    "\n",
    "    # Given the outputs from \"decision_tree\" and \"logistic_regression\"\n",
    "    # the component \"show_results\" is called to print the results.\n",
    "    show_results(accuracy_file, hyperparametres_file, accuracy_dc, accuracy_rf, accuracy_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classify()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9v-dorKaiSOq"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "deploy_config": {},
   "docker_image": "gcr.io/arrikto/jupyter-kale-py38@sha256:2e1ce3427b780c0c78e7cfec527ee10c391092fdc4a8344cd76f8b83c61c5234",
   "experiment_name": "classify-04-01-no-import",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "test-04-01-no-import",
   "snapshot_volumes": true,
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "project-workspace-nbpcf",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
