{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: WorldWeatherPy in /home/jovyan/.local/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.0.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (0.23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from WorldWeatherPy->-r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 4)) (2022.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy->-r requirements.txt (line 1)) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy->-r requirements.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->WorldWeatherPy->-r requirements.txt (line 1)) (1.26.12)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from WorldWeatherPy import DetermineListOfAttributes\n",
    "from WorldWeatherPy import RetrieveByAttribute\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "block:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "def parametres(out_file):\n",
    "    \n",
    "    api_key = 'bd35020cdd3643f4b69142436222912'\n",
    "    attributes = ['date','time', 'moon_illumination', \n",
    "              'tempC', 'tempF', 'windspeedMiles', 'windspeedKmph', 'winddirDegree', 'weatherCode',\n",
    "              'weatherDesc', 'precipMM', 'precipInches', 'humidity', 'visibility', 'visibilityMiles', \n",
    "              'pressure', 'pressureInches', 'cloudcover', 'HeatIndexC', 'HeatIndexF', 'DewPointC', 'DewPointF', 'WindChillC', \n",
    "              'WindChillF', 'WindGustMiles', 'WindGustKmph', 'FeelsLikeC', 'FeelsLikeF', 'uvIndex']\n",
    "    \n",
    "    conditions = ['Sunny','Clear','Cloudy','Rain','Snow']\n",
    "    location_list = ['milan','turin','florence','bologna','rome','naples','palermo']\n",
    "\n",
    "    frequency = 6\n",
    "    start_date = '2018-1-1' \n",
    "    end_date = '2023-1-1'\n",
    "    \n",
    "    parametres = {'api_key' : api_key,\n",
    "            'attributes' : attributes,\n",
    "            'conditions' : conditions,\n",
    "            'location_list' : location_list,\n",
    "            'frequency' : frequency,\n",
    "            'start_date' : start_date,\n",
    "            'end_date' : end_date}\n",
    "\n",
    "    # Creates a json object based on `parametres`\n",
    "    parametres_json = json.dumps(parametres)\n",
    "\n",
    "\n",
    "    # Saves the json object into a file\n",
    "    with open(out_file, 'w') as f:\n",
    "        json.dump(parametres_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "block:milan_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "def download_milan_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][0] #milan\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    \n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "\n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "block:turin_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "def download_turin_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][1] # turin\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "            \n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "block:florence_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "def download_florence_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][2] # florence\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "            \n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "block:bologna_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "def download_bologna_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][3] # bologna\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "\n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "block:rome_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "def download_rome_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][4] # rome\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "            \n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "block:naples_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "def download_naples_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][5] # naples\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "\n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "block:palermo_data",
     "prev:parametres"
    ]
   },
   "outputs": [],
   "source": [
    "def download_palermo_data(parametres_file):\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location = parametres['location_list'][6] # palermo\n",
    "    \n",
    "    dataset = RetrieveByAttribute(parametres['api_key'], parametres['attributes'], location, \n",
    "                                  parametres['start_date'], parametres['end_date'], parametres['frequency']).retrieve_hist_data()\n",
    "    dataset.to_csv(f'data/{location}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    data = pd.read_csv(f'data/{location}.csv')\n",
    "    tmp = []\n",
    "    conditions = parametres['conditions']\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        found = False\n",
    "        for j in conditions:\n",
    "            if j.lower() in row['weatherDesc'].lower():\n",
    "                data.at[i, 'weatherDesc'] = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "                tmp.append(i)\n",
    "                \n",
    "    data = data.drop(tmp)\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(f'data/{location}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "block:merge_data",
     "prev:milan_data",
     "prev:turin_data",
     "prev:florence_data",
     "prev:bologna_data",
     "prev:rome_data",
     "prev:naples_data",
     "prev:palermo_data"
    ]
   },
   "outputs": [],
   "source": [
    "def merge_data(parametres_file):\n",
    "    \n",
    "    download_milan_data(parametres_file)\n",
    "    download_turin_data(parametres_file)\n",
    "    download_florence_data(parametres_file)\n",
    "    download_bologna_data(parametres_file)\n",
    "    download_rome_data(parametres_file)\n",
    "    download_naples_data(parametres_file)\n",
    "    download_palermo_data(parametres_file)\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(parametres_file) as f:\n",
    "        parametres = json.load(f)\n",
    "        \n",
    "    parametres = json.loads(parametres)\n",
    "    \n",
    "    location_list = parametres['location_list']\n",
    "    #data = [None] * len(location_list)\n",
    "\n",
    "    #for i, location in enumerate(location_list):\n",
    "        #data[i] = pd.read_csv(f'datavol-1/data/{location}.csv')\n",
    "        \n",
    "    data = [pd.read_csv(f'data/{location}.csv') for location in location_list]    \n",
    "    \n",
    "    \n",
    "    # creazione della tabella finale utilizzando il metodo concat()\n",
    "    merged_table = pd.concat([data[0], data[1], data[2], data[3], data[4], data[5], data[6]])\n",
    "    merged_table[ ['date','time', 'moon_illumination', \n",
    "              'tempC', 'tempF', 'windspeedMiles', 'windspeedKmph', 'winddirDegree', 'weatherCode', \n",
    "              'weatherDesc', 'precipMM', 'precipInches', 'humidity', 'visibility', 'visibilityMiles', \n",
    "              'pressure', 'pressureInches', 'cloudcover', 'HeatIndexC', 'HeatIndexF', 'DewPointC', 'DewPointF', 'WindChillC', \n",
    "              'WindChillF', 'WindGustMiles', 'WindGustKmph', 'FeelsLikeC', 'FeelsLikeF', 'uvIndex']] = merged_table[ ['date','time','moon_illumination', \n",
    "                                                                                                                      'tempC', 'tempF', 'windspeedMiles', 'windspeedKmph', 'winddirDegree', 'weatherCode', \n",
    "                                                                                                                      'weatherDesc', 'precipMM', 'precipInches', 'humidity', 'visibility', 'visibilityMiles', \n",
    "                                                                                                                      'pressure', 'pressureInches', 'cloudcover', 'HeatIndexC', 'HeatIndexF', 'DewPointC', 'DewPointF', 'WindChillC', \n",
    "                                                                                                                      'WindChillF', 'WindGustMiles', 'WindGustKmph', 'FeelsLikeC', 'FeelsLikeF', 'uvIndex']].replace(-0,0)\n",
    "    merged_table = merged_table.reset_index(drop=True)\n",
    "    merged_table.to_csv('data/merged_table.csv', index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "PB2oCORZO_7O",
    "tags": [
     "block:data_preprocessing",
     "prev:merge_data"
    ]
   },
   "outputs": [],
   "source": [
    "def data_preprocessing():\n",
    "    # lettura di un file CSV in un DataFrame\n",
    "    df = pd.read_csv('data/merged_table.csv')\n",
    "\n",
    "    #df = df.drop(['date','time'], axis=1) #La città potrebbe essere una feature più importante, poiché il meteo può variare significativamente da una città all'altra\n",
    "    df = df.drop(['date','time','city'], axis=1) # Tuttavia, se si utilizza un modello di Random Forest su un dataset di diverse città, il modello potrebbe essere in grado di apprendere autonomamente queste differenze e quindi la colonna 'city' potrebbe non essere necessaria.\n",
    "\n",
    "    y = df[['weatherDesc']]\n",
    "    x = df.drop(['weatherDesc'], axis=1)\n",
    "    columns = x.columns\n",
    "\n",
    "    # Standardize the data in X using a scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x)\n",
    "    X = scaler.transform(x)\n",
    "    features = pd.DataFrame(X, columns = columns)\n",
    "    \n",
    "    dump(scaler, 'models/scaler.joblib')\n",
    "    \n",
    "    return features, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "block:hyperparametres"
    ]
   },
   "outputs": [],
   "source": [
    "def hyperparametres(out_file):\n",
    "    \n",
    "    n_estimators = 400\n",
    "    bootstrap = True\n",
    "    max_features = 'sqrt'\n",
    "    criterion = ['gini','entropy']\n",
    "    min_samples_split = 2 \n",
    "    min_samples_leaf = 1\n",
    "    max_depth = 3\n",
    "\n",
    "    penalty = ['l1','l2']\n",
    "    C = 2.0\n",
    "    solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag']\n",
    "    max_iter = 2000\n",
    "\n",
    "    hyperparametres = {'n_estimators' : n_estimators,\n",
    "            'bootstrap' : bootstrap,\n",
    "            'max_features' : max_features,\n",
    "            'criterion' : criterion,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'penalty' : penalty,\n",
    "            'C' : C,\n",
    "            'solver' : solver,\n",
    "            'max_iter' : max_iter\n",
    "             }\n",
    "\n",
    "    # Creates a json object based on `parametres`\n",
    "    hyperparametres_json = json.dumps(hyperparametres)\n",
    "\n",
    "\n",
    "    # Saves the json object into a file\n",
    "    with open(out_file, 'w') as f:\n",
    "        json.dump(hyperparametres_json, f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "block:decision_tree",
     "prev:data_preprocessing"
    ]
   },
   "outputs": [],
   "source": [
    "def decision_tree(hyperparametres_file):\n",
    "\n",
    "    features, y = data_preprocessing()\n",
    "\n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(hyperparametres_file) as f:\n",
    "        hyperparametres = json.load(f)\n",
    "        \n",
    "    hyperparametres = json.loads(hyperparametres)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, y, test_size = 0.3)\n",
    "\n",
    "    criterion = hyperparametres['criterion'][1] #entropy\n",
    "\n",
    "    model = DecisionTreeClassifier(max_depth = hyperparametres['max_depth'], criterion = criterion)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the model\n",
    "    dump(model, 'models/decision_tree.joblib')\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "block:random_forest",
     "prev:data_preprocessing"
    ]
   },
   "outputs": [],
   "source": [
    "def random_forest(hyperparametres_file):\n",
    "    \n",
    "    features, y = data_preprocessing()\n",
    "    \n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(hyperparametres_file) as f:\n",
    "        hyperparametres = json.load(f)\n",
    "        \n",
    "    hyperparametres = json.loads(hyperparametres)\n",
    "    \n",
    "    criterion = hyperparametres['criterion'][1] #entropy\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, y, test_size = 0.3)\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    model = RandomForestClassifier(n_estimators = hyperparametres['n_estimators'], max_features = hyperparametres['max_features'], criterion = criterion, max_depth = hyperparametres['max_depth'] )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the model\n",
    "    dump(model, 'models/random_forest_model_entropy.joblib')\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "block:logistic_regression",
     "prev:data_preprocessing"
    ]
   },
   "outputs": [],
   "source": [
    "def logistic_regression(hyperparametres_file):\n",
    "\n",
    "    features, y = data_preprocessing()\n",
    "\n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(hyperparametres_file) as f:\n",
    "        hyperparametres = json.load(f)\n",
    "        \n",
    "    hyperparametres = json.loads(hyperparametres)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, y, test_size = 0.3)\n",
    "\n",
    "    penalty = hyperparametres['penalty'][1] #l2\n",
    "    solver = hyperparametres['solver'][0] #newton-cg\n",
    "\n",
    "    model = LogisticRegression(penalty = penalty , C = hyperparametres['C'], solver = solver , max_iter = hyperparametres['max_iter'])\n",
    "    #model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the model\n",
    "    dump(model, 'models/logistic_regression.joblib')\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "block:show_results",
     "prev:random_forest",
     "prev:hyperparametres",
     "prev:decision_tree",
     "prev:logistic_regression"
    ]
   },
   "outputs": [],
   "source": [
    "def show_results(accuracy_file, hyperparametres_file, accuracy_dc, accuracy_rf, accuracy_lr):\n",
    "    # Given the outputs from decision_tree and logistic regression components\n",
    "    # the results are shown.\n",
    "    # Save output into file\n",
    "\n",
    "\n",
    "    # Open and reads file \"parametres\"\n",
    "    with open(hyperparametres_file) as f:\n",
    "        hyperparametres = json.load(f)\n",
    "        \n",
    "    hyperparametres = json.loads(hyperparametres)\n",
    "\n",
    "\n",
    "    with open(accuracy_file, 'w') as f:\n",
    "        f.write('Decision treee(accuracy): ' + str(accuracy_dc) + '  max_depth: ' +  str(hyperparametres['max_depth']) +  ' criterion: ' + hyperparametres['criterion'][1] + '\\n')\n",
    "        f.write('Random forest(accuracy): ' + str(accuracy_rf) + '  n_estimators: ' + str(hyperparametres['n_estimators']) + ' max_features: ' + str(hyperparametres['max_features']) + ' criterion: ' + hyperparametres['criterion'][1] + '\\n')\n",
    "        f.write('Logistic_regression(accuracy): ' + str(accuracy_lr) + '  penalty: '  + hyperparametres['penalty'][1] + ' C: ' + str(hyperparametres['C']) + ' solver: '  + hyperparametres['solver'][2] + ' max_iter: '+ str(hyperparametres['max_iter']) + '\\n')\n",
    "\n",
    "    print(f\"'Decision treee(accuracy): {accuracy_dc} \")\n",
    "    print(f\"Random forest(accuracy): {accuracy_rf} \")\n",
    "    print(f\"Logistic_regression(accuracy): {accuracy_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "block:classify",
     "prev:parametres",
     "prev:merge_data",
     "prev:random_forest",
     "prev:show_results",
     "prev:hyperparametres",
     "prev:decision_tree",
     "prev:logistic_regression"
    ]
   },
   "outputs": [],
   "source": [
    "def classify():\n",
    "    parametres_file = 'parametres.txt'\n",
    "    hyperparametres_file = 'hyperparametres.txt'\n",
    "    accuracy_file = 'results/accuracy.txt'\n",
    "    \n",
    "    parametres(parametres_file)\n",
    "    merge_data(parametres_file)\n",
    "    hyperparametres(hyperparametres_file)\n",
    "    \n",
    "    accuracy_dc = decision_tree(hyperparametres_file)\n",
    "    accuracy_rf = random_forest(hyperparametres_file)\n",
    "    accuracy_lr = logistic_regression(hyperparametres_file)\n",
    "\n",
    "    # Given the outputs from \"decision_tree\" and \"logistic_regression\"\n",
    "    # the component \"show_results\" is called to print the results.\n",
    "    show_results(accuracy_file, hyperparametres_file, accuracy_dc, accuracy_rf, accuracy_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Retrieving weather data for milan\n",
      "\n",
      "\n",
      "Retrieving data for milan from: 2018-01-01 to: 2018-01-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/WorldWeatherPy/by_attribute.py:136: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  list_month_begin = pd.date_range(self.start_date, self.end_date, freq = 'MS', closed = 'right')\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/WorldWeatherPy/by_attribute.py:139: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  list_month_end = pd.date_range(self.start_date_datetime, self.end_date_datetime, freq='M', closed='left')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed (hh:mm:ss.ms) 0:00:01.292311\n",
      "Retrieving data for milan from: 2018-02-01 to: 2018-02-28\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:02.519722\n",
      "Retrieving data for milan from: 2018-03-01 to: 2018-03-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:03.791889\n",
      "Retrieving data for milan from: 2018-04-01 to: 2018-04-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:04.968309\n",
      "Retrieving data for milan from: 2018-05-01 to: 2018-05-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:06.271616\n",
      "Retrieving data for milan from: 2018-06-01 to: 2018-06-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:07.582178\n",
      "Retrieving data for milan from: 2018-07-01 to: 2018-07-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:08.807655\n",
      "Retrieving data for milan from: 2018-08-01 to: 2018-08-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:10.086653\n",
      "Retrieving data for milan from: 2018-09-01 to: 2018-09-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:11.377842\n",
      "Retrieving data for milan from: 2018-10-01 to: 2018-10-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:12.664515\n",
      "Retrieving data for milan from: 2018-11-01 to: 2018-11-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:13.810933\n",
      "Retrieving data for milan from: 2018-12-01 to: 2018-12-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:15.144701\n",
      "Retrieving data for milan from: 2019-01-01 to: 2019-01-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:16.453678\n",
      "Retrieving data for milan from: 2019-02-01 to: 2019-02-28\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:17.599181\n",
      "Retrieving data for milan from: 2019-03-01 to: 2019-03-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:18.844952\n",
      "Retrieving data for milan from: 2019-04-01 to: 2019-04-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:20.034677\n",
      "Retrieving data for milan from: 2019-05-01 to: 2019-05-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:21.207255\n",
      "Retrieving data for milan from: 2019-06-01 to: 2019-06-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:22.405814\n",
      "Retrieving data for milan from: 2019-07-01 to: 2019-07-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:23.657951\n",
      "Retrieving data for milan from: 2019-08-01 to: 2019-08-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:24.842253\n",
      "Retrieving data for milan from: 2019-09-01 to: 2019-09-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:25.892236\n",
      "Retrieving data for milan from: 2019-10-01 to: 2019-10-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:27.128774\n",
      "Retrieving data for milan from: 2019-11-01 to: 2019-11-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:28.323799\n",
      "Retrieving data for milan from: 2019-12-01 to: 2019-12-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:29.649841\n",
      "Retrieving data for milan from: 2020-01-01 to: 2020-01-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:30.990321\n",
      "Retrieving data for milan from: 2020-02-01 to: 2020-02-29\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:32.204406\n",
      "Retrieving data for milan from: 2020-03-01 to: 2020-03-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:33.448342\n",
      "Retrieving data for milan from: 2020-04-01 to: 2020-04-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:34.625297\n",
      "Retrieving data for milan from: 2020-05-01 to: 2020-05-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:35.841339\n",
      "Retrieving data for milan from: 2020-06-01 to: 2020-06-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:37.075185\n",
      "Retrieving data for milan from: 2020-07-01 to: 2020-07-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:38.295402\n",
      "Retrieving data for milan from: 2020-08-01 to: 2020-08-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:39.579741\n",
      "Retrieving data for milan from: 2020-09-01 to: 2020-09-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:40.716164\n",
      "Retrieving data for milan from: 2020-10-01 to: 2020-10-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:41.904813\n",
      "Retrieving data for milan from: 2020-11-01 to: 2020-11-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:43.170330\n",
      "Retrieving data for milan from: 2020-12-01 to: 2020-12-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:44.503668\n",
      "Retrieving data for milan from: 2021-01-01 to: 2021-01-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:45.849218\n",
      "Retrieving data for milan from: 2021-02-01 to: 2021-02-28\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:47.062599\n",
      "Retrieving data for milan from: 2021-03-01 to: 2021-03-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:48.494924\n",
      "Retrieving data for milan from: 2021-04-01 to: 2021-04-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:49.886210\n",
      "Retrieving data for milan from: 2021-05-01 to: 2021-05-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:51.266104\n",
      "Retrieving data for milan from: 2021-06-01 to: 2021-06-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:52.426447\n",
      "Retrieving data for milan from: 2021-07-01 to: 2021-07-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:53.618501\n",
      "Retrieving data for milan from: 2021-08-01 to: 2021-08-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:55.023648\n",
      "Retrieving data for milan from: 2021-09-01 to: 2021-09-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:56.459448\n",
      "Retrieving data for milan from: 2021-10-01 to: 2021-10-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:57.609334\n",
      "Retrieving data for milan from: 2021-11-01 to: 2021-11-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:58.827252\n",
      "Retrieving data for milan from: 2021-12-01 to: 2021-12-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:00.079131\n",
      "Retrieving data for milan from: 2022-01-01 to: 2022-01-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:01.367533\n",
      "Retrieving data for milan from: 2022-02-01 to: 2022-02-28\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:02.644569\n",
      "Retrieving data for milan from: 2022-03-01 to: 2022-03-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:03.901878\n",
      "Retrieving data for milan from: 2022-04-01 to: 2022-04-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:05.203781\n",
      "Retrieving data for milan from: 2022-05-01 to: 2022-05-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:06.497901\n",
      "Retrieving data for milan from: 2022-06-01 to: 2022-06-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:07.838355\n",
      "Retrieving data for milan from: 2022-07-01 to: 2022-07-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:09.146555\n",
      "Retrieving data for milan from: 2022-08-01 to: 2022-08-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:10.449537\n",
      "Retrieving data for milan from: 2022-09-01 to: 2022-09-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:11.570214\n",
      "Retrieving data for milan from: 2022-10-01 to: 2022-10-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:12.908464\n",
      "Retrieving data for milan from: 2022-11-01 to: 2022-11-30\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:14.160378\n",
      "Retrieving data for milan from: 2022-12-01 to: 2022-12-31\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:15.481198\n",
      "Retrieving data for milan from: 2023-01-01 to: 2023-01-01\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:15.668482\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [37], line 7\u001b[0m, in \u001b[0;36mclassify\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m accuracy_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/accuracy.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m parametres(parametres_file)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmerge_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparametres_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m hyperparametres(hyperparametres_file)\n\u001b[1;32m     10\u001b[0m accuracy_dc \u001b[38;5;241m=\u001b[39m decision_tree(hyperparametres_file)\n",
      "Cell \u001b[0;32mIn [30], line 3\u001b[0m, in \u001b[0;36mmerge_data\u001b[0;34m(parametres_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_data\u001b[39m(parametres_file):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mdownload_milan_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparametres_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     download_turin_data(parametres_file)\n\u001b[1;32m      5\u001b[0m     download_florence_data(parametres_file)\n",
      "Cell \u001b[0;32mIn [23], line 18\u001b[0m, in \u001b[0;36mdownload_milan_data\u001b[0;34m(parametres_file)\u001b[0m\n\u001b[1;32m     14\u001b[0m location \u001b[38;5;241m=\u001b[39m parametres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation_list\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#milan\u001b[39;00m\n\u001b[1;32m     16\u001b[0m dataset \u001b[38;5;241m=\u001b[39m RetrieveByAttribute(parametres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m'\u001b[39m], parametres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributes\u001b[39m\u001b[38;5;124m'\u001b[39m], location, \n\u001b[1;32m     17\u001b[0m                               parametres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m], parametres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m'\u001b[39m], parametres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mretrieve_hist_data()\n\u001b[0;32m---> 18\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlocation\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m tmp \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3540\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3542\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3543\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3544\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3548\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3549\u001b[0m )\n\u001b[0;32m-> 3551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_terminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3554\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3556\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/formats/format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1162\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1163\u001b[0m     line_terminator\u001b[38;5;241m=\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1179\u001b[0m )\n\u001b[0;32m-> 1180\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py:694\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 694\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py:568\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    566\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/data'"
     ]
    }
   ],
   "source": [
    "classify()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9v-dorKaiSOq"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "deploy_config": {},
   "docker_image": "gcr.io/arrikto/jupyter-kale-py38@sha256:2e1ce3427b780c0c78e7cfec527ee10c391092fdc4a8344cd76f8b83c61c5234",
   "experiment_name": "classify-04-01",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "test-04-01",
   "snapshot_volumes": true,
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "project-workspace-nbpcf",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
